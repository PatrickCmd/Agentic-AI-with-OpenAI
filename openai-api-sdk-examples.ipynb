{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responses API\n",
    "\n",
    "https://platform.openai.com/docs/quickstart?api-mode=responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"API Key:\", os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a basic API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the heart of the enchanted forest, a shimmering unicorn named Luna softly lulled the stars to sleep with her gentle lullaby, casting dreams of wonder to all who listened.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message.content)\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses API\n",
    "\n",
    "### Generate text from a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a shimmering moon, the unicorn softly trotted through a forest of twinkling stars, leaving trails of dreams for all who slept.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze image inputs\n",
    "### Analyze the content of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows basketball teams from Cleveland and Brooklyn playing against each other.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"what teams are playing in this image?\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/3/3b/LeBron_James_Layup_%28Cleveland_vs_Brooklyn_2018%29.jpg\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the image, it looks like the teams are Liverpool (in red) and a team with a Qatar Airways sponsored jersey, which may be Olympique Lyonnais (OL) or a similar team.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"what teams are playing in this image?\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://media.assettype.com/outlookindia/2025-03-12/dd58gepj/Liverpool-vs-PSG-UEFA-Champions-League-2024-25-Round-Of-16-photos6.jpg?w=640&auto=format%2Ccompress&fit=max&format=webp&dpr=1.0\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a scenic landscape featuring a wooden boardwalk path leading through a lush green field. The sky is bright and blue, with scattered clouds, suggesting a pleasant day. Trees and shrubs can be seen in the distance, enhancing the tranquil outdoor setting.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the imabe about?\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                    \"detail\": \"high\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the model with tools\n",
    "\n",
    "### Get information for the response from the Internet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of March 12, 2025, here are some positive news stories:\n",
      "\n",
      "- **Miami's Growth in Black-Owned Businesses**: Miami has been recognized as the second-highest city in the U.S. for growth in Black-owned businesses, reflecting the city's commitment to diversity and economic development. ([miamirealtors.com](https://www.miamirealtors.com/2025/03/11/wednesday-march-12-2025-south-florida-real-estate-market-articles/?utm_source=openai))\n",
      "\n",
      "- **Brightline's Recognition for Social Good**: Brightline, the high-speed rail service in Florida, has been ranked No. 1 for Social Good by Fast Company and is among the world's 50 most innovative companies for 2024, highlighting its positive impact on the community. ([miamirealtors.com](https://www.miamirealtors.com/2025/03/11/wednesday-march-12-2025-south-florida-real-estate-market-articles/?utm_source=openai))\n",
      "\n",
      "- **Miami Beach's Vibrant Nightlife**: Miami Beach has been ranked as the No. 2 Best Party City in the U.S. for 2025 by U.S. News & World Report, showcasing its lively entertainment scene. ([miamirealtors.com](https://www.miamirealtors.com/2025/03/11/wednesday-march-12-2025-south-florida-real-estate-market-articles/?utm_source=openai))\n",
      "\n",
      "- **Positive Outlook for Oshkosh Businesses**: The Oshkosh Chamber of Commerce's annual Business Outlook Survey indicates that 87% of local businesses anticipate increased sales in 2025, reflecting a strong and optimistic economic environment. ([thebusinessnews.com](https://thebusinessnews.com/northeast/chamber-business-survey-very-positive-news-for-2025/?utm_source=openai))\n",
      "\n",
      "- **Rebound in the CAC 40 Index**: The CAC 40, France's benchmark stock market index, experienced a rebound based on positive news, indicating renewed investor confidence. ([in.marketscreener.com](https://in.marketscreener.com/quote/index/CAC-40-4941/news/CAC-40-a-rebound-based-on-some-good-news-49308813/?utm_source=openai))\n",
      "\n",
      "These stories highlight various positive developments in business growth, community impact, and economic outlooks as of March 12, 2025. \n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"What was a positive news story from today?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris Saint-Germain (PSG) won the Champions League match against Liverpool on March 11, 2025, at Anfield. The match ended 1-0 in favor of PSG, with Ousmane Demb√©l√© scoring in the 12th minute, leveling the aggregate score at 1-1. The tie proceeded to a penalty shootout, where PSG triumphed 4-1. PSG goalkeeper Gianluigi Donnarumma was instrumental, saving penalty kicks from Darwin N√∫√±ez and Curtis Jones, while PSG successfully converted all their penalties. ([reuters.com](https://www.reuters.com/sports/soccer/paris-st-germain-knock-liverpool-out-champions-league-shootout-2025-03-11/?utm_source=openai))\n",
      "\n",
      "This victory marked a historic achievement for PSG, as they became the first French team to overturn a first-leg deficit at Anfield. ([elpais.com](https://elpais.com/deportes/futbol/2025-03-11/el-psg-de-vitinha-se-impone-en-anfield-y-elimina-al-liverpool-en-los-penaltis.html?utm_source=openai))\n",
      "\n",
      "\n",
      "## PSG's Victory Over Liverpool in Champions League:\n",
      "- [Shootout hero Donnarumma helps PSG knock Liverpool out of Champions League](https://www.reuters.com/sports/soccer/paris-st-germain-knock-liverpool-out-champions-league-shootout-2025-03-11/?utm_source=openai)\n",
      "- [El PSG elimina al Liverpool tras lograr la primera remontada de la historia en Anfield](https://elpais.com/deportes/futbol/2025-03-11/el-psg-de-vitinha-se-impone-en-anfield-y-elimina-al-liverpool-en-los-penaltis.html?utm_source=openai)\n",
      "- [Un √©pico Donnarumma mete al PSG en cuartos](https://as.com/futbol/champions/un-epico-donnarumma-mete-al-psg-en-cuartos-n/?utm_source=openai) \n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"Which team won the match in the Champions League for Liverpool vs PSG yesterday on 11th March 2025?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliver blazing fast AI experiences\n",
    "\n",
    "### Stream server-sent events from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_67d15c8a5ec0819287f6f4a489a2b27307c3a19515053f87', created_at=1741773962.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times fast.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    print(event)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build agents\n",
    "\n",
    "### Handoffs example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish agent\",\n",
    "    instructions=\"You only speak Spanish.\",\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    instructions=\"You only speak English\",\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[spanish_agent, english_agent],\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, input=\"Hello, how are you?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! Estoy bien, gracias. ¬øY t√∫, c√≥mo est√°s?\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner\n",
    "import asyncio\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish agent\",\n",
    "    instructions=\"You only speak Spanish.\",\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    instructions=\"You only speak English\",\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[spanish_agent, english_agent],\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, input=\"Hola, ¬øc√≥mo est√°s?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "# Directly await the coroutine in a Jupyter notebook\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Tokyo is sunny. Is there anything else you'd like to know?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from agents import Agent, Runner, function_tool\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Hello world\",\n",
    "    instructions=\"You are a helpful agent.\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
    "    print(result.final_output)\n",
    "    # The weather in Tokyo is sunny.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation and prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text from a simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'resp_67d16c2e88a08192bccbb913d991d34e019c62cf27052367', 'created_at': 1741777966.0, 'error': None, 'incomplete_details': None, 'instructions': None, 'metadata': {}, 'model': 'gpt-4o-2024-08-06', 'object': 'response', 'output': [{'id': 'msg_67d16c2f0a248192b73a5fdf42222764019c62cf27052367', 'content': [{'annotations': [], 'text': 'In a starlit forest where dreams sparkled like fairy dust, a gentle unicorn named Luna guided lost wishes safely back to the night sky.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}], 'parallel_tool_calls': True, 'temperature': 1.0, 'tool_choice': 'auto', 'tools': [], 'top_p': 1.0, 'max_output_tokens': None, 'previous_response_id': None, 'reasoning': {'effort': None, 'generate_summary': None}, 'status': 'completed', 'text': {'format': {'type': 'text'}}, 'truncation': 'disabled', 'usage': {'input_tokens': 36, 'output_tokens': 30, 'output_tokens_details': {'reasoning_tokens': 0}, 'total_tokens': 66, 'input_tokens_details': {'cached_tokens': 0}}, 'user': None, 'store': True}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_67d16c2e88a08192bccbb913d991d34e019c62cf27052367',\n",
       " 'created_at': 1741777966.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'object': 'response',\n",
       " 'output': [{'id': 'msg_67d16c2f0a248192b73a5fdf42222764019c62cf27052367',\n",
       "   'content': [{'annotations': [],\n",
       "     'text': 'In a starlit forest where dreams sparkled like fairy dust, a gentle unicorn named Luna guided lost wishes safely back to the night sky.',\n",
       "     'type': 'output_text'}],\n",
       "   'role': 'assistant',\n",
       "   'status': 'completed',\n",
       "   'type': 'message'}],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'top_p': 1.0,\n",
       " 'max_output_tokens': None,\n",
       " 'previous_response_id': None,\n",
       " 'reasoning': {'effort': None, 'generate_summary': None},\n",
       " 'status': 'completed',\n",
       " 'text': {'format': {'type': 'text'}},\n",
       " 'truncation': 'disabled',\n",
       " 'usage': {'input_tokens': 36,\n",
       "  'output_tokens': 30,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0},\n",
       "  'total_tokens': 66,\n",
       "  'input_tokens_details': {'cached_tokens': 0}},\n",
       " 'user': None,\n",
       " 'store': True}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a starlit forest where dreams sparkled like fairy dust, a gentle unicorn named Luna guided lost wishes safely back to the night sky.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[0].content[0].text\n",
    "## Agents SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a starlit forest where dreams sparkled like fairy dust, a gentle unicorn named Luna guided lost wishes safely back to the night sky.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message roles and instruction following\n",
    "\n",
    "#### Generate text with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr matey! In JavaScript, semicolons be mostly optional due to automatic semicolon insertion. However, it's a good practice to use 'em to avoid any unexpected troubles with yer code. Best keep those seas smooth and your syntax shipshape! üè¥‚Äç‚ò†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"Talk like a pirate.\",\n",
    "    input=\"Are semicolons optional in JavaScript?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text with messages using different roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, aye, matey! In JavaScript, semicolons be mostly optional due to a tricksy thing called Automatic Semicolon Insertion (ASI). But beware, fer it ain't always smooth sailin'! Leavin' them out could lead ye to murky waters with occasional troubles. Best to use 'em when in doubt to keep yer code shipshape! üè¥‚Äç‚ò†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Talk like a pirate.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Are semicolons optional in JavaScript?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and vision\n",
    "\n",
    "#### Analyze the content of an image - Passing a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a scenic landscape featuring a wooden pathway or boardwalk leading through a lush green field. The sky is bright with wispy clouds, and there are trees or bushes in the background, creating a peaceful natural setting.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": \"what's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the content of an image - Passing a Base64 encoded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a wooden boardwalk through a lush green grassy field under a blue sky with some clouds. The scene is bright and appears to be on a clear day. Trees and bushes are visible in the background.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"./Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple image inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images you've shared appear to be identical. They both depict a scenic landscape featuring a wooden path, lush green fields, and a blue sky with clouds. There doesn't seem to be any notable difference between them.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"What are in these images? Is there any difference between them?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs\n",
    "\n",
    "#### Getting a structured response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"}\n",
    "    ],\n",
    "    text={\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"calendar_event\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"participants\": {\n",
    "                        \"type\": \"array\", \n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"name\", \"date\", \"participants\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "event = json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Science Fair', 'date': 'Friday', 'participants': ['Alice', 'Bob']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Outputs for chain-of-thought math tutoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    text={\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"math_reasoning\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"steps\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"explanation\": { \"type\": \"string\" },\n",
    "                                \"output\": { \"type\": \"string\" }\n",
    "                            },\n",
    "                            \"required\": [\"explanation\", \"output\"],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_answer\": { \"type\": \"string\" }\n",
    "                },\n",
    "                \"required\": [\"steps\", \"final_answer\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "math_reasoning = json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [{'explanation': 'Start by isolating the term with the variable on one side of the equation. Subtract 7 from both sides to keep the equation balanced.',\n",
       "   'output': '8x + 7 - 7 = -23 - 7'},\n",
       "  {'explanation': 'Simplify the equation. The +7 and -7 on the left side cancel each other out, leaving 8x. On the right side, -23 minus 7 equals -30.',\n",
       "   'output': '8x = -30'},\n",
       "  {'explanation': 'Now, isolate x by dividing both sides of the equation by 8, the coefficient of x.',\n",
       "   'output': '8x / 8 = -30 / 8'},\n",
       "  {'explanation': 'Simplify the division. The 8s on the left side cancel out, leaving x. On the right side, -30 divided by 8 equals -3.75.',\n",
       "   'output': 'x = -3.75'}],\n",
       " 'final_answer': 'x = -3.75'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from research papers using Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola_virus_research = \"./ebola_virus_zotero_items.json\"\n",
    "with open(ebola_virus_research, \"r\") as f:\n",
    "    ebola_virus_research_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_text = ebola_virus_research_data[0][\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='REVIEW\n",
      "crossm\n",
      "\n",
      "Convalescent Plasma Therapy for COVID-19: State of the Art\n",
      "Daniele Focosi,a Arthur O. Anderson,b Julian W. Tang,c Marco Tuccorid,e\n",
      "aNorth-Western Tuscany Blood Bank, Pisa University Hospital, Pisa, Italy bDepartment of Respiratory Mucosal Immunity, US Army Medical Research Institute of Infectious Diseases, Frederick, Maryland, USA cRespiratory Sciences, University of Leicester, Leicester, United Kingdom dDivision of Pharmacology and Pharmacovigilance, Department of Clinical and Experimental Medicine, University of Pisa, Pisa, Italy eUnit of Adverse Drug Reaction Monitoring, Pisa University Hospital, Pisa, Italy'\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.create_documents([research_text])\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"gpt-4o-mini\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def print_embedding_cost(texts, model_name: str = \"text-embedding-3-small\"):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 29050\n",
      "Embedding Cost in USD: 0.000581\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28875"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = num_tokens_from_string(research_text, encoding_name=\"gpt-4o\")\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REVIEW\\ncrossm\\n\\nConvalescent Plasma Therapy for COVID-19: State of the Art\\nDaniele Focosi,a Arthur O. Anderson,b Julian W. Tang,c Marco Tuccorid,e\\naNorth-Western Tuscany Blood Bank, Pisa University Hospital, Pisa, Italy bDepartment of Respiratory Mucosal Immunity, US Army Medical Research Institute of Infectious Diseases, Frederick, Maryland, USA cRespiratory Sciences, University of Leicester, Leicester, United Kingdom dDivision of Pharmacology and Pharmacovigilance, Department of Clinical and Experimental Medicine, University of Pisa, Pisa, Italy eUnit of Adverse Drug Reaction Monitoring, Pisa University Hospital, Pisa, Italy\\n\\nSUMMARY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 CP DONOR RECRUITMENT STRATEGIES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 CONVALESCENT PLASMA AND PATHOGEN INACTIVATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\nTechnologies To Virally Reduce Plasma (Pathogen Inactivation) . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nLarge-pool products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 MPFS into immunoglobulins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 CP BANKING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 LESSONS FROM SARS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 LESSONS FROM MERS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 CONVALESCENT PLASMA FOR COVID-19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 MONITORING RESPONSE TO CP TREATMENT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 CONCERNS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 SIDE BENEFITS FROM CP IN COVID-19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 CONCLUSIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 AUTHOR BIOS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n\\nSUMMARY Convalescent plasma (CP) therapy has been used since the early 1900s to treat emerging infectious diseases; its efÔ¨Åcacy was later associated with the evidence that polyclonal neutralizing antibodies can reduce the duration of viremia. Recent large outbreaks of viral diseases for which effective antivirals or vaccines are still lacking has renewed the interest in CP as a life-saving treatment. The ongoing COVID-19 pandemic has led to the scaling up of CP therapy to unprecedented levels. Compared with historical usage, pathogen reduction technologies have now added an extra layer of safety to the use of CP, and new manufacturing approaches are being explored. This review summarizes historical settings of application, with a focus on betacoronaviruses, and surveys current approaches for donor selection and CP collection, pooling technologies, pathogen inactivation systems, and banking of CP. We additionally list the ongoing registered clinical trials for CP throughout the world and discuss the trial results published thus far.\\nKEYWORDS Ebola virus disease, Middle East respiratory syndrome, antibodydependent enhancement, convalescent blood product, convalescent plasma, convalescent whole blood, coronavirus disease 2019, enzyme-linked immunosorbent assay, intravenous immunoglobulins, plaque reduction neutralization test, SARS\\n\\nINTRODUCTION\\nThe recent COVID-19 pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) (1) has demonstrated the fragility of our health systems in tackling emergency situations related to the spread of new infectious agents that require the rapid development of effective care strategies. Unfortunately, there are several potentially pandemic viruses, such as Ô¨Çaviviruses (e.g., West Nile virus [WNV],\\n\\nOctober 2020 Volume 33 Issue 4 e00072-20\\n\\nClinical Microbiology Reviews\\n\\nCitation Focosi D, Anderson AO, Tang JW, Tuccori M. 2020. Convalescent plasma therapy for COVID-19: state of the art. Clin Microbiol Rev 33:e00072-20. https://doi.org/10.1128/CMR .00072-20. Copyright ¬© 2020 American Society for Microbiology. All Rights Reserved. Address correspondence to Daniele Focosi, daniele.focosi@gmail.com. Published 12 August 2020\\ncmr.asm.org 1\\n\\nFocosi et al.\\ndengue virus, and Zika virus) (2), chikungunya virus (3), inÔ¨Çuenza viruses A [e.g., A(H1N1) and A(H5N1)] (4), Ebola virus (EBOV) (5), and respiratory betacoronaviruses (SARS-CoV and Middle East respiratory syndrome-CoV [MERS-CoV]), which could put us in situations very similar to the situation with the current pandemic and which require the development of speciÔ¨Åc intervention protocols.\\nWhile vaccination strategy is undoubtedly a viable goal, development of a vaccine requires a time frame not compatible with an emergency situation. It is also a prophylactic approach that has no use in the therapeutic setting. On the other hand, the use of antivirals is valuable for the therapeutic setting (6, 7). For the limited number of antiviral agents currently available, unless provided free of charge to developing countries, Ô¨Ånancial cost is an issue. Additionally, manufacturing is hard to scale up in short time frames.\\nIn situations in which the new pathogen is able to induce an immune response with the production of neutralizing antibodies, passive transfusion of convalescent blood products (CBPs), in particular, convalescent plasma (CP), has proven to be a winning and logistically feasible therapeutic strategy (8). CBPs can be manufactured by collecting whole blood or apheresis plasma from a convalescent donor. This approach has been used since 1900 (9), and previous experiences have been reported elsewhere (10).\\nThe main accepted mechanism of action for CBP therapy is clearance of viremia, which typically happens 10 to 14 days after infection (11). So CBP has been typically administered after the appearance of early symptoms to maximize efÔ¨Åcacy. Convalescent whole blood (CWB), in addition to antibodies, provides control of hemorrhagic events, as in Ebola virus disease, if transfusion occurs within 24 h to maintain viable platelets and clotting factors. Nevertheless, CP best Ô¨Åts settings where only antibodies are required.\\nIn this review, we have described current technologies for CP collection, manufacturing, pathogen inactivation, and banking of CP. Then we have summarized historical settings of CBP application, with a speciÔ¨Åc focus on applications for COVID-19 and other future pandemics. Several articles included in this review are available as preprints which have not yet passed peer review, as indicated in the reference section.\\nCP DONOR RECRUITMENT STRATEGIES Convalescent donor testing for neutralizing antibodies is mandatory in upstream\\ndonor selection. Donor selection is generally based on neutralizing antibody titer, as assessed with a plaque reduction neutralization test (PRNT) (12), which requires a viable isolate, replication-competent cell lines, and skilled personnel. Since PRNT takes time to be set up and requires expensive facilities, in resource-poor settings or in time-sensitive scenarios, collection based on a retrospective PRNT or, alternatively, on an enzymelinked immunosorbent assay (ELISA) targeting the recombinant receptor binding domains (RBDs) of the viral antireceptor has often been implemented; under these circumstances, studies have suggested that ELISA ratios/indexes have good correlations with PRNT titers; e.g., the Euroimmun ELISA IgG score detected 60% of samples with PRNT titers of œæ1:100, with 100% speciÔ¨Åcity using a signal/cutoff reactivity index of 9.1 (13). The current understanding of neutralization suggests that the virus-blocking effect is related to the amount of antibodies against different epitopes coating the virion, whose stoichiometry is in turn affected by antibody concentration and afÔ¨Ånity.\\nThe donor should preferably live in the same area as the intended recipient(s) to allow consideration of mutations of the target viral antigens. SARS-CoV-2 S protein has already mutated after a few months of viral circulation (14), with one mutation outside the receptor-binding motif (23403A¬°G single nucleotide polymorphism, corresponding to a D614G amino acid change) currently deÔ¨Åning a dominant clade (15) characterized by reduced S1 shedding and increased infectivity (16). Nevertheless, it should be considered that preferring indigenous donors could represent a drawback in areas with epidemics of other infecti'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_text[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{research_text[:10000]}\"}\n",
    "    ],\n",
    "    # max_output_tokens=30000,\n",
    "    text={\n",
    "        \"format\": {\n",
    "              \"type\": \"json_schema\",\n",
    "              \"name\": \"research_paper_extraction\",\n",
    "              \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"title\": { \"type\": \"string\" },\n",
    "                    \"doi\": { \"type\": \"string\" },\n",
    "                    \"authors\": { \n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": { \"type\": \"string\" }\n",
    "                    },\n",
    "                    \"abstract\": { \"type\": \"string\" },\n",
    "                    \"keywords\": { \n",
    "                        \"type\": \"array\", \n",
    "                        \"items\": { \"type\": \"string\" }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"title\", \"doi\", \"authors\", \"abstract\", \"keywords\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "research_paper = json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Convalescent Plasma Therapy for COVID-19: State of the Art',\n",
       " 'doi': '10.1128/CMR.00072-20',\n",
       " 'authors': ['Daniele Focosi',\n",
       "  'Arthur O. Anderson',\n",
       "  'Julian W. Tang',\n",
       "  'Marco Tuccori'],\n",
       " 'abstract': 'Convalescent plasma (CP) therapy has been used since the early 1900s to treat emerging infectious diseases; its efficacy was later associated with the evidence that polyclonal neutralizing antibodies can reduce the duration of viremia. Recent large outbreaks of viral diseases for which effective antivirals or vaccines are still lacking have renewed the interest in CP as a life-saving treatment. The ongoing COVID-19 pandemic has led to the scaling up of CP therapy to unprecedented levels. Compared with historical usage, pathogen reduction technologies have now added an extra layer of safety to the use of CP, and new manufacturing approaches are being explored. This review summarizes historical settings of application, with a focus on betacoronaviruses, and surveys current approaches for donor selection and CP collection, pooling technologies, pathogen inactivation systems, and banking of CP. We additionally list the ongoing registered clinical trials for CP throughout the world and discuss the trial results published thus far.',\n",
       " 'keywords': ['Ebola virus disease',\n",
       "  'Middle East respiratory syndrome',\n",
       "  'antibody-dependent enhancement',\n",
       "  'convalescent blood product',\n",
       "  'convalescent plasma',\n",
       "  'convalescent whole blood',\n",
       "  'coronavirus disease 2019',\n",
       "  'enzyme-linked immunosorbent assay',\n",
       "  'intravenous immunoglobulins',\n",
       "  'plaque reduction neutralization test',\n",
       "  'SARS']}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refusals with Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedChatCompletionMessage[MathReasoning](content='{\"steps\":[{\"explanation\":\"First, we need to isolate the term with x on one side of the equation. To do this, we can subtract 7 from both sides of the equation to eliminate the 7 on the left.\",\"output\":\"8x + 7 - 7 = -23 - 7\"},{\"explanation\":\"Simplifying both sides, the left side becomes 8x (because 7 - 7 = 0), and the right side becomes -30.\",\"output\":\"8x = -30\"},{\"explanation\":\"Next, to solve for x, we divide both sides by 8.\",\"output\":\"8x / 8 = -30 / 8\"},{\"explanation\":\"Simplifying the division on the right side, we get x = -30 / 8, which can be reduced to x = -15 / 4 after dividing the numerator and denominator by their greatest common divisor, which is 2.\",\"output\":\"x = -15 / 4\"}],\"final_answer\":\"x = -15/4\"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=MathReasoning(steps=[Step(explanation='First, we need to isolate the term with x on one side of the equation. To do this, we can subtract 7 from both sides of the equation to eliminate the 7 on the left.', output='8x + 7 - 7 = -23 - 7'), Step(explanation='Simplifying both sides, the left side becomes 8x (because 7 - 7 = 0), and the right side becomes -30.', output='8x = -30'), Step(explanation='Next, to solve for x, we divide both sides by 8.', output='8x / 8 = -30 / 8'), Step(explanation='Simplifying the division on the right side, we get x = -30 / 8, which can be reduced to x = -15 / 4 after dividing the numerator and denominator by their greatest common divisor, which is 2.', output='x = -15 / 4')], final_answer='x = -15/4'))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '{\"steps\":[{\"explanation\":\"First, we need to isolate the term with x on one side of the equation. To do this, we can subtract 7 from both sides of the equation to eliminate the 7 on the left.\",\"output\":\"8x + 7 - 7 = -23 - 7\"},{\"explanation\":\"Simplifying both sides, the left side becomes 8x (because 7 - 7 = 0), and the right side becomes -30.\",\"output\":\"8x = -30\"},{\"explanation\":\"Next, to solve for x, we divide both sides by 8.\",\"output\":\"8x / 8 = -30 / 8\"},{\"explanation\":\"Simplifying the division on the right side, we get x = -30 / 8, which can be reduced to x = -15 / 4 after dividing the numerator and denominator by their greatest common divisor, which is 2.\",\"output\":\"x = -15 / 4\"}],\"final_answer\":\"x = -15/4\"}',\n",
       " 'refusal': None,\n",
       " 'role': 'assistant',\n",
       " 'annotations': [],\n",
       " 'audio': None,\n",
       " 'function_call': None,\n",
       " 'tool_calls': None,\n",
       " 'parsed': {'steps': [{'explanation': 'First, we need to isolate the term with x on one side of the equation. To do this, we can subtract 7 from both sides of the equation to eliminate the 7 on the left.',\n",
       "    'output': '8x + 7 - 7 = -23 - 7'},\n",
       "   {'explanation': 'Simplifying both sides, the left side becomes 8x (because 7 - 7 = 0), and the right side becomes -30.',\n",
       "    'output': '8x = -30'},\n",
       "   {'explanation': 'Next, to solve for x, we divide both sides by 8.',\n",
       "    'output': '8x / 8 = -30 / 8'},\n",
       "   {'explanation': 'Simplifying the division on the right side, we get x = -30 / 8, which can be reduced to x = -15 / 4 after dividing the numerator and denominator by their greatest common divisor, which is 2.',\n",
       "    'output': 'x = -15 / 4'}],\n",
       "  'final_answer': 'x = -15/4'}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_reasoning.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=[Step(explanation='First, we need to isolate the term with x on one side of the equation. To do this, we can subtract 7 from both sides of the equation to eliminate the 7 on the left.', output='8x + 7 - 7 = -23 - 7'), Step(explanation='Simplifying both sides, the left side becomes 8x (because 7 - 7 = 0), and the right side becomes -30.', output='8x = -30'), Step(explanation='Next, to solve for x, we divide both sides by 8.', output='8x / 8 = -30 / 8'), Step(explanation='Simplifying the division on the right side, we get x = -30 / 8, which can be reduced to x = -15 / 4 after dividing the numerator and denominator by their greatest common divisor, which is 2.', output='x = -15 / 4')] final_answer='x = -15/4'\n"
     ]
    }
   ],
   "source": [
    "# If the model refuses to respond, you will get a refusal message\n",
    "if (math_reasoning.refusal):\n",
    "    print(math_reasoning.refusal)\n",
    "else:\n",
    "    print(math_reasoning.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [{'explanation': 'First, we need to isolate the term with x on one side of the equation. To do this, we can subtract 7 from both sides of the equation to eliminate the 7 on the left.',\n",
       "   'output': '8x + 7 - 7 = -23 - 7'},\n",
       "  {'explanation': 'Simplifying both sides, the left side becomes 8x (because 7 - 7 = 0), and the right side becomes -30.',\n",
       "   'output': '8x = -30'},\n",
       "  {'explanation': 'Next, to solve for x, we divide both sides by 8.',\n",
       "   'output': '8x / 8 = -30 / 8'},\n",
       "  {'explanation': 'Simplifying the division on the right side, we get x = -30 / 8, which can be reduced to x = -15 / 4 after dividing the numerator and denominator by their greatest common divisor, which is 2.',\n",
       "   'output': 'x = -15 / 4'}],\n",
       " 'final_answer': 'x = -15/4'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(math_reasoning.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"attributes\":[\"quick\",\"brown\",\"lazy\",\"piercing\",\"blue\"],\"colors\":[\"brown\",\"blue\"],\"animals\":[\"fox\",\"dog\"]}Completed\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract entities from the input text\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The quick brown fox jumps over the lazy dog with piercing blue eyes\"\n",
    "        },\n",
    "    ],\n",
    "    text={\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"entities\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"attributes\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"colors\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"animals\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"attributes\", \"colors\", \"animals\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    if event.type == 'response.refusal.delta':\n",
    "        print(event.delta, end=\"\")\n",
    "    elif event.type == 'response.output_text.delta':\n",
    "        print(event.delta, end=\"\")\n",
    "    elif event.type == 'response.error':\n",
    "        print(event.error, end=\"\")\n",
    "    elif event.type == 'response.completed':\n",
    "        print(\"Completed\")\n",
    "        # print(event.response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "#### Function calling example with get_weather function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(id='fc_67d1c003f4f0819296b4622fc2aed26f0fb4fe78ab74d2d0', arguments='{\"location\":\"Paris, France\"}', call_id='call_J8umf9Yy4Xb4HTJ7SVPrr2OC', name='get_weather', type='function_call', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Bogot√°, Colombia\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"location\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling example with search_knowledge_base function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(id='fc_67d1c0ffa90481928ab33563059fb4ee07b2ff28d8667e3b', arguments='{\"query\":\"ChatGPT\",\"options\":{\"num_results\":5,\"domain_filter\":\"AI\",\"sort_by\":\"relevance\"}}', call_id='call_0Dx9xXWfvuIrz5d0EpxCsMV6', name='search_knowledge_base', type='function_call', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search_knowledge_base\",\n",
    "    \"description\": \"Query a knowledge base to retrieve relevant info on a topic.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user question or search query.\"\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"num_results\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Number of top results to return.\"\n",
    "                    },\n",
    "                    \"domain_filter\": {\n",
    "                        \"type\": [\n",
    "                            \"string\",\n",
    "                            \"null\"\n",
    "                        ],\n",
    "                        \"description\": \"Optional domain to narrow the search (e.g. 'finance', 'medical'). Pass null if not needed.\"\n",
    "                    },\n",
    "                    \"sort_by\": {\n",
    "                        \"type\": [\n",
    "                            \"string\",\n",
    "                            \"null\"\n",
    "                        ],\n",
    "                        \"enum\": [\n",
    "                            \"relevance\",\n",
    "                            \"date\",\n",
    "                            \"popularity\",\n",
    "                            \"alphabetical\"\n",
    "                        ],\n",
    "                        \"description\": \"How to sort results. Pass null if not needed.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"num_results\",\n",
    "                    \"domain_filter\",\n",
    "                    \"sort_by\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"query\",\n",
    "            \"options\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"Can you find information about ChatGPT in the AI knowledge base?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling example with send_email function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(id='fc_67d1c1877e508192baed44699fff5dec06a3d8f3cfab4792', arguments='{\"to\":\"ilan@example.com\",\"subject\":\"Hello!\",\"body\":\"Hi Ilan!\"}', call_id='call_zpncxZ79ZYoK8vvOQWgJPJfB', name='send_email', type='function_call', status='completed'), ResponseFunctionToolCall(id='fc_67d1c18810a08192a62d36f6c925e8a406a3d8f3cfab4792', arguments='{\"to\":\"katia@example.com\",\"subject\":\"Hello!\",\"body\":\"Hi Katia!\"}', call_id='call_XTL39ZPrP7GdaXMuogcrAvti', name='send_email', type='function_call', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"send_email\",\n",
    "    \"description\": \"Send an email to a given recipient with a subject and message.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"to\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The recipient email address.\"\n",
    "            },\n",
    "            \"subject\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Email subject line.\"\n",
    "            },\n",
    "            \"body\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Body of the email message.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"to\",\n",
    "            \"subject\",\n",
    "            \"body\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"Can you send an email to ilan@example.com and katia@example.com saying hi?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample get_weather function implemented in your codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Call model with get_weather tool defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"latitude\": {\"type\": \"number\"},\n",
    "            \"longitude\": {\"type\": \"number\"}\n",
    "        },\n",
    "        \"required\": [\"latitude\", \"longitude\"],\n",
    "        \"additionalProperties\": False\n",
    "    },\n",
    "    \"strict\": True\n",
    "}]\n",
    "\n",
    "input_messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris today?\"}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=input_messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_67d1c28ab13081928ea8ae896b2198660ff7c6afa369a084', created_at=1741800074.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseFunctionToolCall(id='fc_67d1c28b5fac8192b7c1530511d94f0f0ff7c6afa369a084', arguments='{\"latitude\":48.8566,\"longitude\":2.3522}', call_id='call_tL6Svmf1oj9XY8O2Eubn6Pl9', name='get_weather', type='function_call', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='get_weather', parameters={'type': 'object', 'properties': {'latitude': {'type': 'number'}, 'longitude': {'type': 'number'}}, 'required': ['latitude', 'longitude'], 'additionalProperties': False}, strict=True, type='function', description='Get current temperature for provided coordinates in celsius.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=269, output_tokens=25, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=294, input_tokens_details={'cached_tokens': 0}), user=None, store=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(id='fc_67d1c28b5fac8192b7c1530511d94f0f0ff7c6afa369a084', arguments='{\"latitude\":48.8566,\"longitude\":2.3522}', call_id='call_tL6Svmf1oj9XY8O2Eubn6Pl9', name='get_weather', type='function_call', status='completed')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute get_weather function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 48.8566, 'longitude': 2.3522}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = response.output[0]\n",
    "args = json.loads(tool_call.arguments)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.9"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_weather(args[\"latitude\"], args[\"longitude\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply result and call model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Paris is 7.9¬∞C.\n"
     ]
    }
   ],
   "source": [
    "input_messages.append(tool_call)  # append model's function call message\n",
    "input_messages.append({                               # append result message\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": tool_call.call_id,\n",
    "    \"output\": str(result)\n",
    "})\n",
    "\n",
    "response_2 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=input_messages,\n",
    "    tools=tools,\n",
    ")\n",
    "print(response_2.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in tools\n",
    "\n",
    "### Web Search\n",
    "\n",
    "#### Include web search results for the model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, Wednesday, March 12, 2025, the UEFA Champions League Round of 16 second-leg matches are scheduled as follows:\n",
      "\n",
      "- **Lille vs. Borussia Dortmund**: Kick-off at 12:45 PM ET (11:45 AM CT, 10:45 AM MT, 9:45 AM PT) at Stade Pierre-Mauroy in Lille, France. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
      "\n",
      "- **Arsenal vs. PSV Eindhoven**: Kick-off at 3:00 PM ET (2:00 PM CT, 1:00 PM MT, 12:00 PM PT) at Emirates Stadium in London, UK. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
      "\n",
      "- **Aston Villa vs. Club Brugge**: Kick-off at 3:00 PM ET (2:00 PM CT, 1:00 PM MT, 12:00 PM PT) at Villa Park in Birmingham, UK. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
      "\n",
      "- **Atl√©tico Madrid vs. Real Madrid**: Kick-off at 3:00 PM ET (2:00 PM CT, 1:00 PM MT, 12:00 PM PT) at Metropolitano Stadium in Madrid, Spain. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
      "\n",
      "These matches will determine which teams advance to the quarterfinals of the competition.\n",
      "\n",
      "\n",
      "## Key Champions League Matches Today:\n",
      "- [Atl√©tico de Madrid - Real Madrid: horario y d√≥nde ver el partido de Champions League](https://elpais.com/deportes/futbol/2025-03-12/horario-derbi-atletico-de-madrid-real-madrid-y-donde-ver-el-partido-de-la-uefa-champions-league.html?utm_source=openai)\n",
      "- [Arsenal - PSV: Horario, TV; c√≥mo y d√≥nde ver en USA la Champions League](https://as.com/us/futbol/arsenal-psv-horario-tv-como-y-donde-ver-en-usa-la-champions-league-n/?utm_source=openai)\n",
      "- [Lille - Borussia Dortmund, en directo: octavos de Champions hoy en vivo](https://as.com/futbol/champions/lille-borussia-dortmund-en-directo-octavos-de-champions-hoy-en-vivo-n/?utm_source=openai) \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"What games are to be played for the Champions League fixtures today March 12th 2025?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Today, Wednesday, March 12, 2025, the UEFA Champions League Round of 16 second-leg matches are scheduled as follows:\n",
       "\n",
       "- **Lille vs. Borussia Dortmund**: Kick-off at 12:45 PM ET (11:45 AM CT, 10:45 AM MT, 9:45 AM PT) at Stade Pierre-Mauroy in Lille, France. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
       "\n",
       "- **Arsenal vs. PSV Eindhoven**: Kick-off at 3:00 PM ET (2:00 PM CT, 1:00 PM MT, 12:00 PM PT) at Emirates Stadium in London, UK. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
       "\n",
       "- **Aston Villa vs. Club Brugge**: Kick-off at 3:00 PM ET (2:00 PM CT, 1:00 PM MT, 12:00 PM PT) at Villa Park in Birmingham, UK. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
       "\n",
       "- **Atl√©tico Madrid vs. Real Madrid**: Kick-off at 3:00 PM ET (2:00 PM CT, 1:00 PM MT, 12:00 PM PT) at Metropolitano Stadium in Madrid, Spain. ([beinsports.com](https://www.beinsports.com/en-us/soccer/uefa-champions-league/articles-video/uefa-champions-league-2025-round-of-16-dates-and-kickoff-times-in-the-united-states-2025-02-21?utm_source=openai))\n",
       "\n",
       "These matches will determine which teams advance to the quarterfinals of the competition.\n",
       "\n",
       "\n",
       "## Key Champions League Matches Today:\n",
       "- [Atl√©tico de Madrid - Real Madrid: horario y d√≥nde ver el partido de Champions League](https://elpais.com/deportes/futbol/2025-03-12/horario-derbi-atletico-de-madrid-real-madrid-y-donde-ver-el-partido-de-la-uefa-champions-league.html?utm_source=openai)\n",
       "- [Arsenal - PSV: Horario, TV; c√≥mo y d√≥nde ver en USA la Champions League](https://as.com/us/futbol/arsenal-psv-horario-tv-como-y-donde-ver-en-usa-la-champions-league-n/?utm_source=openai)\n",
       "- [Lille - Borussia Dortmund, en directo: octavos de Champions hoy en vivo](https://as.com/futbol/champions/lille-borussia-dortmund-en-directo-octavos-de-champions-hoy-en-vivo-n/?utm_source=openai) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File search\n",
    "\n",
    "#### Create a vector store and upload a file\n",
    "#### Upload the file to the File API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-TgC96pX4Lmpu9GrZY3mkxx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def create_file(client, file_path):\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(\n",
    "            file=file_tuple,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(\n",
    "                file=file_content,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "    print(result.id)\n",
    "    return result.id\n",
    "\n",
    "# Replace with your own file path or URL\n",
    "file_id = create_file(client, \"https://cdn.openai.com/API/docs/deep_research_blog.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_67d1d69e7c348191b6e824e379d404f7\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.vector_stores.create(\n",
    "    name=\"knowledge_base\"\n",
    ")\n",
    "print(vector_store.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'vs_67d1d69e7c348191b6e824e379d404f7',\n",
       "   'created_at': 1741805214,\n",
       "   'file_counts': {'cancelled': 0,\n",
       "    'completed': 1,\n",
       "    'failed': 0,\n",
       "    'in_progress': 0,\n",
       "    'total': 1},\n",
       "   'last_active_at': 1741805327,\n",
       "   'metadata': {},\n",
       "   'name': 'knowledge_base',\n",
       "   'object': 'vector_store',\n",
       "   'status': 'completed',\n",
       "   'usage_bytes': 66539,\n",
       "   'expires_after': None,\n",
       "   'expires_at': None}],\n",
       " 'has_more': False,\n",
       " 'object': 'list',\n",
       " 'first_id': 'vs_67d1d69e7c348191b6e824e379d404f7',\n",
       " 'last_id': 'vs_67d1d69e7c348191b6e824e379d404f7'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.vector_stores.list().model_dump()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the file to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStoreFile(id='file-TgC96pX4Lmpu9GrZY3mkxx', created_at=1741805328, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_67d1d69e7c348191b6e824e379d404f7', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))\n"
     ]
    }
   ],
   "source": [
    "result = client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file_id\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check status\n",
    "Run this code until the file is ready to be used (i.e., when the status is completed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[VectorStoreFile](data=[VectorStoreFile(id='file-TgC96pX4Lmpu9GrZY3mkxx', created_at=1741805295, last_error=None, object='vector_store.file', status='completed', usage_bytes=66539, vector_store_id='vs_67d1d69e7c348191b6e824e379d404f7', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))], has_more=False, object='list', first_id='file-TgC96pX4Lmpu9GrZY3mkxx', last_id='file-TgC96pX4Lmpu9GrZY3mkxx')\n"
     ]
    }
   ],
   "source": [
    "result = client.vector_stores.files.list(\n",
    "    vector_store_id=vector_store.id\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       " 'created_at': 1741805295,\n",
       " 'last_error': None,\n",
       " 'object': 'vector_store.file',\n",
       " 'status': 'completed',\n",
       " 'usage_bytes': 66539,\n",
       " 'vector_store_id': 'vs_67d1d69e7c348191b6e824e379d404f7',\n",
       " 'attributes': {},\n",
       " 'chunking_strategy': {'static': {'chunk_overlap_tokens': 400,\n",
       "   'max_chunk_size_tokens': 800},\n",
       "  'type': 'static'}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.data[0].model_dump()\n",
    "#### File search tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_67d1d7c264ec81929b1e396bbfff6fdd079ebfd3e99956ea', created_at=1741805506.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFileSearchToolCall(id='fs_67d1d7c34668819297c72ea4dbb2e7c7079ebfd3e99956ea', queries=['deep research by OpenAI', 'What is deep research by OpenAI?', 'OpenAI deep research definition', 'OpenAI research initiatives'], status='completed', type='file_search_call', results=None), ResponseOutputMessage(id='msg_67d1d7c81af081928b0dc43a014dad78079ebfd3e99956ea', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=508, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=765, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=765, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1002, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1002, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1178, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1401, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1600, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1868, type='file_citation', filename='deep_research_blog.pdf'), AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1868, type='file_citation', filename='deep_research_blog.pdf')], text='Deep Research by OpenAI is a newly launched capability within ChatGPT designed to conduct extensive, agentic research on the internet. It allows users to efficiently complete complex research tasks that traditionally require significant human effort and time. Here are some key features and functions of Deep Research:\\n\\n1. **Multi-Step Research**: It synthesizes large amounts of information from various online sources, quickly assembling a comprehensive report that resembles the work of a research analyst.\\n\\n2. **Reasoning and Browsing**: The system leverages advanced reasoning capabilities to search, interpret, and analyze text, images, and PDF documents available on the internet. It can pivot its approach based on real-time data encountered during research.\\n\\n3. **Target Users**: Deep Research is tailored for individuals involved in intensive knowledge work, such as finance, science, policy, and engineering, as well as for general consumers seeking detailed information on specific products.\\n\\n4. **Documentation and Citation**: Every output generated is fully documented with clear citations and summaries, facilitating easy reference and verification of information.\\n\\n5. **Training Methodology**: The model was trained using advanced reinforcement learning techniques specifically tailored for complex browsing and reasoning tasks, enhancing its ability to perform detailed investigations.\\n\\n6. **Performance Metrics**: The model has demonstrated substantial advancements in benchmark tests, outperforming previous AI systems in accuracy and reasoning across a range of expert-level tasks.\\n\\nThe capacity for Deep Research to operate independently, synthesize findings, and provide detailed reports is seen as a significant step toward OpenAI‚Äôs broader goal of developing artificial general intelligence (AGI) capable of generating novel scientific research.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_67d1d69e7c348191b6e824e379d404f7'], filters=None, max_num_results=20, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=17183, output_tokens=429, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=17612, input_tokens_details={'cached_tokens': 0}), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"What is deep research by OpenAI?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_67d1d7c264ec81929b1e396bbfff6fdd079ebfd3e99956ea',\n",
       " 'created_at': 1741805506.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'object': 'response',\n",
       " 'output': [{'id': 'fs_67d1d7c34668819297c72ea4dbb2e7c7079ebfd3e99956ea',\n",
       "   'queries': ['deep research by OpenAI',\n",
       "    'What is deep research by OpenAI?',\n",
       "    'OpenAI deep research definition',\n",
       "    'OpenAI research initiatives'],\n",
       "   'status': 'completed',\n",
       "   'type': 'file_search_call',\n",
       "   'results': None},\n",
       "  {'id': 'msg_67d1d7c81af081928b0dc43a014dad78079ebfd3e99956ea',\n",
       "   'content': [{'annotations': [{'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 508,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 765,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 765,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1002,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1002,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1178,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1401,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1600,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1868,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'},\n",
       "      {'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1868,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'}],\n",
       "     'text': 'Deep Research by OpenAI is a newly launched capability within ChatGPT designed to conduct extensive, agentic research on the internet. It allows users to efficiently complete complex research tasks that traditionally require significant human effort and time. Here are some key features and functions of Deep Research:\\n\\n1. **Multi-Step Research**: It synthesizes large amounts of information from various online sources, quickly assembling a comprehensive report that resembles the work of a research analyst.\\n\\n2. **Reasoning and Browsing**: The system leverages advanced reasoning capabilities to search, interpret, and analyze text, images, and PDF documents available on the internet. It can pivot its approach based on real-time data encountered during research.\\n\\n3. **Target Users**: Deep Research is tailored for individuals involved in intensive knowledge work, such as finance, science, policy, and engineering, as well as for general consumers seeking detailed information on specific products.\\n\\n4. **Documentation and Citation**: Every output generated is fully documented with clear citations and summaries, facilitating easy reference and verification of information.\\n\\n5. **Training Methodology**: The model was trained using advanced reinforcement learning techniques specifically tailored for complex browsing and reasoning tasks, enhancing its ability to perform detailed investigations.\\n\\n6. **Performance Metrics**: The model has demonstrated substantial advancements in benchmark tests, outperforming previous AI systems in accuracy and reasoning across a range of expert-level tasks.\\n\\nThe capacity for Deep Research to operate independently, synthesize findings, and provide detailed reports is seen as a significant step toward OpenAI‚Äôs broader goal of developing artificial general intelligence (AGI) capable of generating novel scientific research.',\n",
       "     'type': 'output_text'}],\n",
       "   'role': 'assistant',\n",
       "   'status': 'completed',\n",
       "   'type': 'message'}],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [{'type': 'file_search',\n",
       "   'vector_store_ids': ['vs_67d1d69e7c348191b6e824e379d404f7'],\n",
       "   'filters': None,\n",
       "   'max_num_results': 20,\n",
       "   'ranking_options': {'ranker': 'auto', 'score_threshold': 0.0}}],\n",
       " 'top_p': 1.0,\n",
       " 'max_output_tokens': None,\n",
       " 'previous_response_id': None,\n",
       " 'reasoning': {'effort': None, 'generate_summary': None},\n",
       " 'status': 'completed',\n",
       " 'text': {'format': {'type': 'text'}},\n",
       " 'truncation': 'disabled',\n",
       " 'usage': {'input_tokens': 17183,\n",
       "  'output_tokens': 429,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0},\n",
       "  'total_tokens': 17612,\n",
       "  'input_tokens_details': {'cached_tokens': 0}},\n",
       " 'user': None,\n",
       " 'store': True}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval customization\n",
    "Limiting the number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_67d1d8b02a1c81928d4131c88c3160a10b78e124825fcec5', created_at=1741805744.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFileSearchToolCall(id='fs_67d1d8b109dc81928035c6adca2df6f00b78e124825fcec5', queries=['What is deep research by OpenAI?'], status='completed', type='file_search_call', results=None), ResponseOutputMessage(id='msg_67d1d8b2dc2c819297c54c38bb9299fb0b78e124825fcec5', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='file-TgC96pX4Lmpu9GrZY3mkxx', index=1075, type='file_citation', filename='deep_research_blog.pdf')], text=\"Deep research by OpenAI is a new feature in ChatGPT that allows users to conduct multi-step research tasks independently. This capability is designed to synthesize large amounts of information from various online sources and generate comprehensive reports similar to what a research analyst would produce. \\n\\nKey aspects of deep research include:\\n\\n1. **Agentic Capability**: It performs tasks autonomously by finding, analyzing, and synthesizing information from hundreds of sources on the internet.\\n2. **Efficiency**: It accomplishes in a fraction of the time what would typically take a human many hours.\\n3. **Applications**: It's particularly beneficial for users in fields like finance, science, policy, and engineering, as well as for consumers seeking personalized product recommendations.\\n4. **Documentation**: Every output is well-cited, making it easy to reference and verify the data.\\n5. **Advanced Training**: The model powering deep research utilizes extensive training on reasoning and browsing tasks, enhancing its ability to tackle complex, real-world inquiries. \\n\\nThis feature aims to streamline the research process, making it faster and more accurate for various professional and personal needs.\", type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_67d1d69e7c348191b6e824e379d404f7'], filters=None, max_num_results=2, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=3540, output_tokens=254, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=3794, input_tokens_details={'cached_tokens': 0}), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"What is deep research by OpenAI?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "        \"max_num_results\": 2\n",
    "    }]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_67d1d8b02a1c81928d4131c88c3160a10b78e124825fcec5',\n",
       " 'created_at': 1741805744.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'object': 'response',\n",
       " 'output': [{'id': 'fs_67d1d8b109dc81928035c6adca2df6f00b78e124825fcec5',\n",
       "   'queries': ['What is deep research by OpenAI?'],\n",
       "   'status': 'completed',\n",
       "   'type': 'file_search_call',\n",
       "   'results': None},\n",
       "  {'id': 'msg_67d1d8b2dc2c819297c54c38bb9299fb0b78e124825fcec5',\n",
       "   'content': [{'annotations': [{'file_id': 'file-TgC96pX4Lmpu9GrZY3mkxx',\n",
       "       'index': 1075,\n",
       "       'type': 'file_citation',\n",
       "       'filename': 'deep_research_blog.pdf'}],\n",
       "     'text': \"Deep research by OpenAI is a new feature in ChatGPT that allows users to conduct multi-step research tasks independently. This capability is designed to synthesize large amounts of information from various online sources and generate comprehensive reports similar to what a research analyst would produce. \\n\\nKey aspects of deep research include:\\n\\n1. **Agentic Capability**: It performs tasks autonomously by finding, analyzing, and synthesizing information from hundreds of sources on the internet.\\n2. **Efficiency**: It accomplishes in a fraction of the time what would typically take a human many hours.\\n3. **Applications**: It's particularly beneficial for users in fields like finance, science, policy, and engineering, as well as for consumers seeking personalized product recommendations.\\n4. **Documentation**: Every output is well-cited, making it easy to reference and verify the data.\\n5. **Advanced Training**: The model powering deep research utilizes extensive training on reasoning and browsing tasks, enhancing its ability to tackle complex, real-world inquiries. \\n\\nThis feature aims to streamline the research process, making it faster and more accurate for various professional and personal needs.\",\n",
       "     'type': 'output_text'}],\n",
       "   'role': 'assistant',\n",
       "   'status': 'completed',\n",
       "   'type': 'message'}],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [{'type': 'file_search',\n",
       "   'vector_store_ids': ['vs_67d1d69e7c348191b6e824e379d404f7'],\n",
       "   'filters': None,\n",
       "   'max_num_results': 2,\n",
       "   'ranking_options': {'ranker': 'auto', 'score_threshold': 0.0}}],\n",
       " 'top_p': 1.0,\n",
       " 'max_output_tokens': None,\n",
       " 'previous_response_id': None,\n",
       " 'reasoning': {'effort': None, 'generate_summary': None},\n",
       " 'status': 'completed',\n",
       " 'text': {'format': {'type': 'text'}},\n",
       " 'truncation': 'disabled',\n",
       " 'usage': {'input_tokens': 3540,\n",
       "  'output_tokens': 254,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0},\n",
       "  'total_tokens': 3794,\n",
       "  'input_tokens_details': {'cached_tokens': 0}},\n",
       " 'user': None,\n",
       " 'store': True}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deep research by OpenAI is a new feature in ChatGPT that allows users to conduct multi-step research tasks independently. This capability is designed to synthesize large amounts of information from various online sources and generate comprehensive reports similar to what a research analyst would produce. \n",
       "\n",
       "Key aspects of deep research include:\n",
       "\n",
       "1. **Agentic Capability**: It performs tasks autonomously by finding, analyzing, and synthesizing information from hundreds of sources on the internet.\n",
       "2. **Efficiency**: It accomplishes in a fraction of the time what would typically take a human many hours.\n",
       "3. **Applications**: It's particularly beneficial for users in fields like finance, science, policy, and engineering, as well as for consumers seeking personalized product recommendations.\n",
       "4. **Documentation**: Every output is well-cited, making it easy to reference and verify the data.\n",
       "5. **Advanced Training**: The model powering deep research utilizes extensive training on reasoning and browsing tasks, enhancing its ability to tackle complex, real-world inquiries. \n",
       "\n",
       "This feature aims to streamline the research process, making it faster and more accurate for various professional and personal needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
